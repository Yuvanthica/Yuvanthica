{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn-oXELFL9ye"
      },
      "source": [
        "\n",
        "## Installations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyZ3hj-6MBQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40df9eca-da82-48e8-bc8f-3ee2e3649c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUMV_Iz1Wex"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzV563gm1crK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import bs4 as bs\n",
        "import torch\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzd5gXUkZzVM"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34PhgFQS8GbV"
      },
      "source": [
        "## Get data from Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BCrKTDin8JOV",
        "outputId": "858a316d-da21-41c0-e9af-11458879e8d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eb11da2cf6d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/businessballs_data_disagg.csv'\u001b[0m \u001b[0;31m# change path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m data = pd.read_csv(file_path,engine='python', encoding='utf-8', \n\u001b[0;32m----> 6\u001b[0;31m                     error_bad_lines=False, sep = separator)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"readline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/businessballs_data_disagg.csv'"
          ]
        }
      ],
      "source": [
        "# read scraped data\n",
        "separator = ';' # change here, it depends on the file loaded\n",
        "\n",
        "file_path = '/content/businessballs_data_disagg.csv' # change path here \n",
        "data = pd.read_csv(file_path,engine='python', encoding='utf-8', \n",
        "                    error_bad_lines=False, sep = separator)\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UVZ9G0MF71e"
      },
      "source": [
        "# 1) Zero Shot classification\n",
        "Performing zero shot classification on paragraphs to check the **relevancy** of their content with respect to the associated soft skill (from data scraped on task 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7QWg4zogD20"
      },
      "outputs": [],
      "source": [
        "# TODO : work on scraped_data and generate a data frame with the same name for the next step\n",
        "\n",
        "# Read the file\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "df_wiki = pd.read_csv('/content/scraped_data_disagg.csv') #For testing purposes \n",
        "paragraphs = df_wiki['paragraph'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tYxtEgvn8NT"
      },
      "outputs": [],
      "source": [
        "# We'll use a subset to test out the computational time of model/API\n",
        "paragraphs_subset = paragraphs[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxRNLJqcF71e"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "import heapq\n",
        "\n",
        "# Zero shot labels\n",
        "skill_labels = ['Teamwork', 'Problem-solving', 'Communication', 'Self-Awareness', 'Time management',\n",
        "              'Critical thinking', 'Decision-making', 'Organizational', 'Stress management',\n",
        "              'Adaptability', 'Conflict management', 'Leadership', 'Creativity',\n",
        "              'Resourcefulness', 'Persuasion', 'Openness to criticism', 'Confidence',\n",
        "              'Mutual respect', 'Empathy', 'Active listening', 'Constructive feedback', 'Collaboration',\n",
        "              'Negotiation', 'Irrelevant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_hLrcnOjLLr"
      },
      "outputs": [],
      "source": [
        "# Testing using a loaded model from the Huggingface pipeline\n",
        "classifier = pipeline (\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfC5yX3CfTtb"
      },
      "outputs": [],
      "source": [
        "# Loop over paragraphs and perform zero-shot classification\n",
        "t0 = time.time()\n",
        "\n",
        "for i in range(len(paragraphs)):\n",
        "  par = paragraphs[i]\n",
        "\n",
        "  result = classifier(par, skill_labels)\n",
        "  scores = result['scores']\n",
        "  output_labels = result['labels']\n",
        "  index = scores.index(max(scores)) #  Or np.argmax(result['scores'], axis=0)\n",
        "  \n",
        "  df_wiki.loc[i,'Zero-Shot Label'] = output_labels[index]\n",
        "\n",
        "model_time = format_time(time.time() - t0)\n",
        "\n",
        "print(\"\")\n",
        "print(\"  Performing ZSC using an instantiated model took: {:}\".format(model_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfEIyhBCjKX0"
      },
      "outputs": [],
      "source": [
        "# Testing using the API\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "    \"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n",
        "    \"parameters\": {\"candidate_labels\": [\"refund\", \"legal\", \"faq\"]},\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-ef54Ggkam-"
      },
      "outputs": [],
      "source": [
        "# Sneak peek at the generated labels\n",
        "print(df_wiki[['Soft Skill Name', 'Zero-Shot Label']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fewn0T70O0cR"
      },
      "outputs": [],
      "source": [
        "df_wiki.to_csv('scraped_data_zeroshot.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B0OvY4mHOK0"
      },
      "outputs": [],
      "source": [
        "  # In case we want to perform multi-class zero-shot classification\n",
        "  \n",
        "  # result = classifier(par, skill_labels, multi_label=True)['scores']\n",
        "  # index = heapq.nlargest(3, range(len(result)), key=result.__getitem__)\n",
        "  # print(\"labels : \", heapq.nlargest(3, result))\n",
        "  # for j in index:\n",
        "  #   zs_label = zs_label +', '+ skill_labels[j]\n",
        "\n",
        "  # df_wiki.loc[i,'Zero-Shot Label'] = zs_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlT345lhLspw"
      },
      "source": [
        "## 2) Vectorization of the Paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0pWQaABLxxn"
      },
      "outputs": [],
      "source": [
        "paragraphs = data['paragraph'].values\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iob05P49L26V"
      },
      "outputs": [],
      "source": [
        "#Get maximum length from the paragraphs\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for par in paragraphs:\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(par, add_special_tokens=True)\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFKk6RuyM0SF"
      },
      "outputs": [],
      "source": [
        "bert.eval()\n",
        "\n",
        "paragraph_vectors = []\n",
        "for par in paragraphs:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        par,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids = encoded_dict['input_ids']\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks = encoded_dict['attention_mask']\n",
        "\n",
        "    # Encode the sentence using BERT\n",
        "    with torch.no_grad():\n",
        "      outputs = bert(input_ids, attention_mask=attention_masks)\n",
        "      hidden_states = outputs[2] # Get BERT's 12 output values\n",
        "\n",
        "    # A simple approach for generating the whole paragraph embedding\n",
        "    # is to average the second to last hiden layer of each token producing a single 768 length vector.\n",
        "    token_vecs = hidden_states[-2][0]\n",
        "    # Calculate the average of all token vectors of the current paragraph.\n",
        "    paragraph_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "    paragraph_vectors.append(paragraph_embedding.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nht-B6mQNCEN"
      },
      "outputs": [],
      "source": [
        "data['Vector'] = paragraph_vectors\n",
        "data.to_csv('businessballsdata_withVectors.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRcQBCamO7B5"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Mee5Eb0QL6"
      },
      "source": [
        "# 3) Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ZGMLiALZD-"
      },
      "source": [
        "### a) KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UCqANLhLbqu"
      },
      "outputs": [],
      "source": [
        "skill_labels = ['Teamwork', 'Problem-solving', 'Communication', 'Self-Awareness', 'Time management',\n",
        "              'Critical thinking', 'Decision-making', 'Organizational', 'Stress management',\n",
        "              'Adaptability', 'Conflict management', 'Leadership', 'Creativity',\n",
        "              'Resourcefulness', 'Persuasion', 'Openness to criticism', 'Confidence',\n",
        "              'Mutual respect', 'Empathy', 'Active listening', 'Constructive feedback', 'Collaboration',\n",
        "              'Negotiation', 'Irrelevant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVlkWkzT1NIP"
      },
      "outputs": [],
      "source": [
        "# Experimenting with different values for k (based on the number of soft skills we have)\n",
        "Sum_of_squared_distances = []\n",
        "K = range(2,len(skill_labels))\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
        "    km = km.fit(paragraph_vectors)\n",
        "    Sum_of_squared_distances.append(km.inertia_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9quAQE-NKih"
      },
      "outputs": [],
      "source": [
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aA9O8goNSNQ"
      },
      "outputs": [],
      "source": [
        "true_k = 6\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\n",
        "model.fit(paragraph_vectors)\n",
        "labels=model.labels_\n",
        "\n",
        "par_cl=pd.DataFrame(list(zip(paragraphs,labels)),columns=['paragraph','cluster'])\n",
        "print(par_cl.sort_values(by=['cluster']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PREDQU295xH4"
      },
      "outputs": [],
      "source": [
        "data['Cluster ID'] = par_cl['cluster']\n",
        "data.to_csv('wikidata_withClusters.csv')\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqeMpZq0NW7k"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "scatter_plot_points = pca.fit_transform(np.array(paragraph_vectors))\n",
        "\n",
        "colors = [\"r\", \"b\", \"c\", \"y\", \"g\", \"m\"]\n",
        "\n",
        "x_axis = [o[0] for o in scatter_plot_points]\n",
        "y_axis = [o[1] for o in scatter_plot_points]\n",
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "\n",
        "ax.scatter(x_axis, y_axis, c=[colors[d] for d in labels])\n",
        "\n",
        "for i, txt in enumerate(data['Soft Skill Name']):\n",
        "    ax.annotate(txt, (x_axis[i], y_axis[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjBANNqLuNP1"
      },
      "source": [
        "To hopefully illustrate things further more, here are the clusters that we can find here: \n",
        "\n",
        "- Cluster 1 (Red) : (5 paragraphs referring to **collaboration**, 3 referring to **self awareness**, 1 referring to **creativity**, 1 referring to **communication**)\n",
        "- Cluster 2 (Yellow) : (3 paragraphs referring to **creativity**, 2 referring to **communication**, 1 referring to **self awareness**)\n",
        "- Cluster 3 (Cyan) : (4 paragraphs referring to **communication**, 1 referring to **creativity**)\n",
        "- Cluster 4 (Purple) : 2 paragraphs referring to **self-awareness**\n",
        "- Cluster 5 (Green) : 2 paragraphs referring to **self-awareness** \n",
        "(Notice that paragraphs in cluster 4 and 5 all talk about self awareness, but they've been clustered differently according to their actual/more detailed content)\n",
        "- Cluster 6 (Blue) : (1 paragraph referring to **creativity**, 1 referring to **communication**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqQ2CsRyaSxL"
      },
      "source": [
        "# 3) Content summarization\n",
        "Summarize each paragraph to facilitate assigning human-readable labels to them\n",
        "\n",
        "**NOTE : the output of this task will ONLY be used for manual labeling, and will not be provided to further tasks.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/scraped_data_clustering.csv') # to be commented if we we want to use data from the previous step (have to run all steps before !)"
      ],
      "metadata": {
        "id": "Jcv-XUYjpZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = list(data['paragraph'].values) # to be commented if we want to the one defined in previous steps"
      ],
      "metadata": {
        "id": "4rM3A_5NvOzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = list(data['cluser'].unique())\n",
        "clusters"
      ],
      "metadata": {
        "id": "VKLibscYvnJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDD4iLdVaSbp"
      },
      "outputs": [],
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "# Better to use the API\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_paragraphs(pars): # pars : list of paragraphs\n",
        "  n = len(pars)\n",
        "\n",
        "  if n < 1 :\n",
        "    return ''\n",
        "\n",
        "  if n== 1 :\n",
        "    return query(par[0])[0]['summary_text']\n",
        "\n",
        "  if n == 2 :\n",
        "    summary_1 = query(pars[0])[0]['summary_text']\n",
        "    summary_2 = query(pars[1])[0]['summary_text']\n",
        "    return query(summary_1 + '\\n' + summary_2)[0]['summary_text']\n",
        "  else:\n",
        "    summary_1 = summarize_paragraphs(paragraphs[0:(n//2)])\n",
        "    summary_2  = summarize_paragraphs(paragraphs[(n//2):n])\n",
        "    return query(summary_1 + '\\n' + summary_2)[0]['summary_text']\n"
      ],
      "metadata": {
        "id": "B4c90zKmxrj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function\n",
        "summarize_paragraphs(paragraphs[0:40])"
      ],
      "metadata": {
        "id": "5DrDkLQTxrlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks0JSG8Y6lj5"
      },
      "outputs": [],
      "source": [
        "# Summarize paragraphs per cluster until reaching a size manageable enough for manual labeling\n",
        "summaries = []\n",
        "\n",
        "#The code currently summarizes all paragraphs at once,\n",
        "# To be modified in order to group them by cluster ID\n",
        "for par in paragraphs[0:1]:\n",
        "  input = {\n",
        "      'inputs': par\n",
        "  }\n",
        "  summary = query(input)[0]['summary_text']\n",
        "  summaries.append(summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Paragraph summary'] = summaries"
      ],
      "metadata": {
        "id": "Ln4UUANJy-Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcYxPiJYAHQA"
      },
      "outputs": [],
      "source": [
        "data[['Paragraph', 'Paragraph summary']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAvvNdOrMwEx"
      },
      "outputs": [],
      "source": [
        "data.to_csv('wikidata_withSummaries')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI5C5gF47yp4"
      },
      "source": [
        "# 4) Assign labels for clusters\n",
        "Manually assign the human-readable labels to clusters (multiple paragraphs are assigned to a certain cluster, the objective is to label this cluster as a whole)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_hvg8d273U1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxrK-egJ0nxh"
      },
      "source": [
        "# 5) Zero-shot Classification\n",
        "Perform another round of zero-shot classification on the manually annotated dataset from step n° 2 using the human-readable labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9hTSmw9cVUZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxDCTAfB1N65"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khM5lXJRcd_q"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"oigele/Fb_improved_zeroshot\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgokTgmzcm3V"
      },
      "outputs": [],
      "source": [
        "################### dummy code to check if multi-lables work##################\n",
        "sequence_to_classify = \"natural language processing\"\n",
        "fine_labels = []\n",
        "classifier(sequence_to_classify, candidate_labels, multi_label=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1TBxdy00nuO"
      },
      "source": [
        "# 6) Content Aggregation\n",
        "Aggregate (assemble) paragraphs from the same cluster "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsPnwxkwgk0g"
      },
      "outputs": [],
      "source": [
        "########## Below code is to check if the alogrithm works. Using dummy data for now.#############\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vTD-5Gd1Okn"
      },
      "outputs": [],
      "source": [
        "columns = [\"labels\", \"agg_sum_content\"]\n",
        "rows = [[[\"1\"], \"Some content for label 1\"],\n",
        "        [[\"1\", \"2\"], \"Some content for label 1 and 2\"],\n",
        "        [[\"3\", \"4\"], \"Some content for label 3 and 4\"],\n",
        "        [[\"4\"], \"Some content for label 4\"],\n",
        "        [[\"1\", \"5\", \"2\"], \"Some content for label 1, 2 and 5\"],\n",
        "        [[\"6\"], \"Some content for label 6\"],]\n",
        "dummy_df = pd.DataFrame(rows, columns = columns)\n",
        "dummy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buyuW-7bh4Wy"
      },
      "outputs": [],
      "source": [
        "def aggregate_content_based_on_labels(df):\n",
        "  labels = df['labels'].explode().unique()\n",
        "  new_data = {}\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    curr_labels = row[\"labels\"]\n",
        "    for label in curr_labels:\n",
        "      if label not in new_data:\n",
        "        new_data[label] = []\n",
        "      new_data[label].append(row[\"agg_sum_content\"])\n",
        "      new_data[label] = ['. '.join(new_data[label])]\n",
        "  \n",
        "  print(new_data)\n",
        "  new_df = pd.DataFrame.from_dict(new_data, orient='index',\n",
        "                       columns=['agg_content'])\n",
        "  new_df\n",
        "  return new_df\n",
        "new_df = aggregate_content_based_on_labels(dummy_df)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no7W_NQy0nrW"
      },
      "source": [
        "# 7) Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPOWV6BjtuQS"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNkhDKtw1PIW"
      },
      "outputs": [],
      "source": [
        "#summarizer = pipeline('summarization', model=\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI_7ht2OoYgE"
      },
      "outputs": [],
      "source": [
        "def abstractive_summarize_content(content):\n",
        "    inputs = tokenizer(content, max_length=1024, return_tensors=\"pt\")\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=30)\n",
        "    return tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "new_df['summary'] = new_df['agg_content'].apply(abstractive_summarize_content)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4_vGxQwqnJh"
      },
      "outputs": [],
      "source": [
        "type(new_df[\"agg_content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bducjJg4o6pi"
      },
      "outputs": [],
      "source": [
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aam0qekPJFd5"
      },
      "source": [
        "Summarization -  extractive model with frequency driven approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgCUslpAJBxF"
      },
      "outputs": [],
      "source": [
        "#sentence_list = nltk.sent_tokenize(article_text)\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import heapq\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "max_words_per_sentance=15\n",
        "max_sentances_in_summary=2\n",
        "\n",
        "\n",
        "def extractive_summarize_content(article_text):\n",
        "  \n",
        "  formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
        "  formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
        "  sentence_list = nltk.sent_tokenize(article_text)\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  word_frequencies = {}\n",
        "  for word in nltk.word_tokenize(formatted_article_text):\n",
        "    if word not in stopwords:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "  maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        " \n",
        "  for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "  sentence_scores = {}\n",
        "  for sent in sentence_list:\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies.keys():\n",
        "            if len(sent.split(' ')) < max_words_per_sentance:\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]\n",
        "  summary_sentences = heapq.nlargest(max_sentances_in_summary, sentence_scores, key=sentence_scores.get)\n",
        "  summary = ' '.join(summary_sentences)\n",
        "  \n",
        "  return summary\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function can be used to call the actuall summary function based upon a string input, can make this better"
      ],
      "metadata": {
        "id": "MOXjcic-uiUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summary(type):\n",
        "  if \"extractive\" in type:\n",
        "    return extractive_summarize_content\n",
        " # if \"abstractive\" in type:\n",
        "  #  return abstractive_summarize_content\n",
        "  return abstractive_summarize_content\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QRIu87TTuYyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below part is to run the extractive summarization function and display the output. Could be removed later"
      ],
      "metadata": {
        "id": "pubX1WcifkAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"India is a great country. India's capital is New Delhi. It's prime minister is Narendra Modi. He is from Gujarat.\",\"Virat Kohli is one of the most succesful cricketers of all time. He has scored most hundereds among current players. Despite a bad patch he is still scoring runs and broke his century drought by scoreing a 100 against Afghanistan.\",\"I am a software engineer with a unicorn startup. Before that I was in Consultancy and Analytics. I didnt go for an MBA as I was not ready for a commitment. So I am upskilling and looking for other oppurtunities.\"]\n",
        "  \n",
        "\n",
        "df = pd.DataFrame(data, columns=['para'])\n",
        "\n",
        "df['summary'] = df['para'].apply(summary(\"extractive\")) \n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "bMZ-fs5XegRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8jvoLvZ0nof"
      },
      "source": [
        "# 8) Ranking of Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixFfBdZT1Pik"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOWxKW130nl4"
      },
      "source": [
        "# 9) Saving the data to a database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6W8FJHJz87F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}